*** Begin Patch

*** Add File: cloud/edge/openai.js

// cloud/edge/openai.js
// Edge proxy for OpenAI with mock mode reading fixtures.
// - Uses process.env.OPENAI_API_KEY for real calls
// - When process.env.OPENAI_MOCK === 'true' returns responses from fixtures
// - Supports: POST /ai/openai/chat  and POST /ai/openai/transcribe
/* eslint-disable no-console */
import fs from 'fs';
import crypto from 'crypto';

const OPENAI_KEY = process.env.OPENAI_API_KEY;
const OPENAI_BASE = process.env.OPENAI_BASE || 'https://api.openai.com/v1';
const MOCK = process.env.OPENAI_MOCK === 'true';

function sha256hex(str) {
  return crypto.createHash('sha256').update(str).digest('hex');
}

function requireKey() {
  if (!OPENAI_KEY && !MOCK) throw new Error('Missing OPENAI_API_KEY');
}

async function openaiFetch(path, opts = {}) {
  requireKey();
  const url = `${OPENAI_BASE}${path}`;
  const headers = { Authorization: `Bearer ${OPENAI_KEY}`, ...(opts.headers || {}) };
  return await fetch(url, { ...opts, headers });
}

function loadMockFixtures() {
  try {
    const p = './replay/fixtures/mock_openai_responses.json';
    const raw = fs.readFileSync(p, 'utf8');
    return JSON.parse(raw);
  } catch (e) {
    console.warn('Missing mock fixtures:', e.message);
    return {};
  }
}

export default async function handler(req) {
  try {
    const url = new URL(req.url);
    const pathname = url.pathname.replace(/\/+$/, '');
    // Chat endpoint
    if (pathname.endsWith('/ai/openai/chat') && req.method === 'POST') {
      const body = await req.json();
      if (MOCK) {
        const fixtures = loadMockFixtures();
        // if caller provided a mockKey, return that block
        if (body.mockKey && fixtures.chats && fixtures.chats[body.mockKey]) {
          return new Response(JSON.stringify(fixtures.chats[body.mockKey]), { status: 200, headers: { 'Content-Type': 'application/json' }});
        }
        // fallback: return demo/default
        const fallback = fixtures.chats && fixtures.chats['demo_chat'] ? fixtures.chats['demo_chat'] : { id: 'mock', choices:[{message:{role:'assistant', content:'Mock reply'}}] };
        return new Response(JSON.stringify(fallback), { status: 200, headers: { 'Content-Type': 'application/json' }});
      }

      // real mode
      requireKey();
      const payload = {
        model: body.model || 'gpt-4o-mini',
        messages: body.messages || [],
        max_tokens: body.max_tokens ?? 512,
        temperature: body.temperature ?? 0.7,
        stream: !!body.stream
      };

      if (payload.stream) {
        const resp = await openaiFetch('/chat/completions', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload)
        });
        if (!resp.ok) {
          const txt = await resp.text();
          return new Response(txt, { status: resp.status, headers: { 'Content-Type': 'text/plain' }});
        }
        // proxy streaming body to client
        const reader = resp.body.getReader();
        const stream = new ReadableStream({
          async pull(controller) {
            const { done, value } = await reader.read();
            if (done) { controller.close(); return; }
            controller.enqueue(value);
          }
        });
        return new Response(stream, { status: 200, headers: { 'Content-Type': 'text/event-stream' }});
      } else {
        const resp = await openaiFetch('/chat/completions', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload)
        });
        if (!resp.ok) {
          const txt = await resp.text();
          return new Response(txt, { status: resp.status, headers: { 'Content-Type': 'text/plain' }});
        }
        const json = await resp.json();
        return new Response(JSON.stringify(json), { status: 200, headers: { 'Content-Type': 'application/json' }});
      }
    }

    // Transcription endpoint
    if (pathname.endsWith('/ai/openai/transcribe') && req.method === 'POST') {
      const body = await req.json();
      if (MOCK) {
        const fixtures = loadMockFixtures();
        if (body.mockKey && fixtures.transcripts && fixtures.transcripts[body.mockKey]) {
          return new Response(JSON.stringify(fixtures.transcripts[body.mockKey]), { status: 200, headers: { 'Content-Type': 'application/json' }});
        }
        const fallback = { text: 'Mock transcription: player says something.' };
        return new Response(JSON.stringify(fallback), { status: 200, headers: { 'Content-Type': 'application/json' }});
      }

      requireKey();
      const audioBuffer = Buffer.from(body.audioBase64, 'base64');
      const formData = new FormData();
      formData.append('file', new Blob([audioBuffer]), 'recording.wav');
      formData.append('model', body.model || 'whisper-1');
      const resp = await openaiFetch('/audio/transcriptions', {
        method: 'POST',
        body: formData
      });
      if (!resp.ok) {
        const txt = await resp.text();
        return new Response(txt, { status: resp.status });
      }
      const json = await resp.json();
      return new Response(JSON.stringify(json), { status: 200, headers: { 'Content-Type': 'application/json' }});
    }

    return new Response('Not found', { status: 404 });
  } catch (err) {
    console.error('openai edge error', err);
    return new Response(JSON.stringify({ error: err.message }), { status: 500, headers: { 'Content-Type': 'application/json' }});
  }
}

*** End Patch

*** Add File: src/ai/openaiClient.ts

// src/ai/openaiClient.ts
// Frontend client that talks to cloud/edge/openai.js
const EDGE_BASE = process.env.NEXT_PUBLIC_EDGE_BASE || '/api';

export async function chatCompletion({ messages, model='gpt-4o-mini', max_tokens=512, temperature=0.7, cache=false, mockKey }: {
  messages: { role: string, content: string }[],
  model?: string,
  max_tokens?: number,
  temperature?: number,
  cache?: boolean,
  mockKey?: string
}) {
  const res = await fetch(`${EDGE_BASE}/ai/openai/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ messages, model, max_tokens, temperature, cache, mockKey })
  });
  if (!res.ok) {
    const txt = await res.text();
    throw new Error(`Chat failed: ${res.status} ${txt}`);
  }
  return res.json();
}

export async function streamChat({ messages, model='gpt-4o-mini', temperature=0.7 } : { messages: any[], model?: string, temperature?: number }) {
  const res = await fetch(`${EDGE_BASE}/ai/openai/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ messages, model, temperature, stream: true })
  });
  if (!res.ok) throw new Error(`Stream chat failed: ${res.status}`);
  return res.body;
}

export async function transcribeAudioBase64(audioBase64: string, mockKey?: string) {
  const res = await fetch(`${EDGE_BASE}/ai/openai/transcribe`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ audioBase64, model: 'whisper-1', mockKey })
  });
  if (!res.ok) {
    const txt = await res.text();
    throw new Error(`Transcription failed: ${res.status} ${txt}`);
  }
  return res.json();
}

*** End Patch

*** Add File: replay/fixtures/mock_openai_responses.json

{
  "chats": {
    "demo_chat": {
      "id": "mock-demo-001",
      "choices": [
        {
          "message": {
            "role": "assistant",
            "content": "Welcome to the demo. In this run your choices shape the biome. Try harvesting or preserving the Bio-Seed."
          }
        }
      ]
    },
    "archive_harvest_attempt": {
      "id": "mock-archive-harvest",
      "choices": [
        {
          "message": {
            "role": "assistant",
            "content": "The earth shudders; miners cheer while Dr. Mara's eyes grow distant."
          }
        }
      ]
    },
    "archive_preserve_confirm": {
      "id": "mock-archive-preserve",
      "choices": [
        {
          "message": {
            "role": "assistant",
            "content": "You bind the roots and slow the extraction. A hush of green returns."
          }
        }
      ]
    },
    "merchant_offer": {
      "id": "mock-merchant-001",
      "choices": [
        {
          "message": {
            "role": "assistant",
            "content": "A traveling merchant offers tech for food. Accepting yields immediate growth, but costs reputation with the Biologists."
          }
        }
      ]
    },
    "epilogue_overclock": {
      "id": "epilogue_overclock",
      "choices": [
        {
          "message": {
            "role": "assistant",
            "content": "Cities rose like glass, humming with stolen nights. The river remembers our hunger."
          }
        }
      ]
    },
    "epilogue_bioconserve": {
      "id": "epilogue_bioconserve",
      "choices": [
        {
          "message": {
            "role": "assistant",
            "content": "Green light stitched the ruins back together. We traded speed for seeds, and the future hummed soft."
          }
        }
      ]
    }
  },
  "transcripts": {
    "sample_speech_1": {
      "text": "Mock transcription: The scouts report a faint hum under the northern vault."
    },
    "player_short": {
      "text": "Mock transcription: Hold the chokepoint, bring up shields."
    }
  },
  "events": {
    "bio_seed_aftershock": {
      "event": "Bio-Seed Aftershock",
      "flavor": "A low, wet sound rolls from below. Dr. Mara whispers, 'It remembers the old world.'",
      "effect": { "wildlifeAggression": 0.30, "localYieldDecay": 0.25, "narrativeTag": "guilt" }
    },
    "artifact_orb": {
      "event": "Found Relic",
      "flavor": "A glass orb hums with old code — half-song, half-warning.",
      "effect": { "artifactCollected": "glass_orb", "loreSeed": "orb_song_01" }
    }
  }
}

*** End Patch

*** Add File: replay/fixtures/prebaked_seeds.json

[
  {
    "id": "seed-archive-001",
    "map": "CrystalValley",
    "mapSeed": 913027,
    "startResources": 80,
    "playerFaction": "Nomad-Guild",
    "bioSeedHealth": 85,
    "hostileDensity": 0.25,
    "recommendedStrategy": "Conserve for long-term buffs; avoid immediate harvest",
    "presetEvents": ["scout_find:north_vault"],
    "notes": "Demo: Ethical choice visible in 10–12 minute run."
  },
  {
    "id": "seed-archive-002",
    "map": "IronRidge",
    "mapSeed": 913043,
    "startResources": 30,
    "playerFaction": "Forge-Consortium",
    "bioSeedHealth": 60,
    "hostileDensity": 0.4,
    "recommendedStrategy": "High pressure; harvesting yields tactical advantage but spawns hazards",
    "presetEvents": ["trader_offer:quick_tech"],
    "notes": "Demo: Short-term harvesting tradeoff; good for contested choice telemetry."
  },
  {
    "id": "seed-archive-003",
    "map": "SiltMarsh",
    "mapSeed": 913128,
    "startResources": 50,
    "playerFaction": "Riverhold",
    "bioSeedHealth": 40,
    "hostileDensity": 0.2,
    "recommendedStrategy": "Fragile bio-seed; preserve to avoid acid bloom",
    "presetEvents": ["bio_seed_sigh"],
    "notes": "Demo: environmental event triggered if harvested; useful for audio demonstration."
  },
  {
    "id": "seed-archive-004",
    "map": "GlassHollows",
    "mapSeed": 913200,
    "startResources": 120,
    "playerFaction": "Skyward",
    "bioSeedHealth": 95,
    "hostileDensity": 0.15,
    "recommendedStrategy": "Resource-rich; opportunity to experiment with BioConserve tech",
    "presetEvents": ["artifact_discovered:glass_orb"],
    "notes": "Demo: rich choices, artifact ties into lore journal; great for narrative epilogue."
  },
  {
    "id": "seed-archive-005",
    "map": "Scarfield",
    "mapSeed": 913400,
    "startResources": 20,
    "playerFaction": "Outriders",
    "bioSeedHealth": 20,
    "hostileDensity": 0.6,
    "recommendedStrategy": "Hard mode; encourages risky harvests and reveals moral consequences",
    "presetEvents": ["storm_trigger:acid_bloom"],
    "notes": "Judge challenge: intended to highlight mechanical vs narrative tension under stress."
  }
]

*** End Patch

*** Add File: tools/generate_mock_audio.js

#!/usr/bin/env node
// tools/generate_mock_audio.js
// Creates mock OGG placeholder files for SSML voice pack so AudioManager can demo playback locally.
const fs = require('fs');
const path = require('path');

const outDir = path.join(process.cwd(), 'public', 'assets', 'tts-mock');
if (!fs.existsSync(outDir)) fs.mkdirSync(outDir, { recursive: true });

const voices = [
  { id: 'lian_1', text: 'Hold the chokepoint — buy us time.' },
  { id: 'lian_2', text: 'We move when I say we move.' },
  { id: 'mara_1', text: 'Please — listen. It remembers more than we do.' },
  { id: 'mara_2', text: 'There must be another way.' },
  { id: 'patch_1', text: 'Alarms: loud. Morale: quieter than you, commander.' },
  { id: 'patch_2', text: 'Scanning... nothing helpful. Sending passive judgement.' }
];

function makeFakeOgg(size = 8192) {
  const header = Buffer.from('OggS');
  const payload = Buffer.alloc(size);
  return Buffer.concat([header, payload]);
}

voices.forEach(v => {
  const fname = path.join(outDir, `${v.id}.ogg`);
  fs.writeFileSync(fname, makeFakeOgg());
  console.log('Wrote', fname);
});

console.log('Mock TTS audio created at', outDir);

*** End Patch

