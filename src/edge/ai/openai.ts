// src/edge/ai/openai.ts
// Lovable Cloud Edge Function: OpenAI API Proxy
// Exposes: POST /api/ai/openai/chat, POST /api/ai/openai/transcribe
// Tool: OpenAI (Sponsor-tag ready)
// Security: Keeps OPENAI_API_KEY server-side only

import crypto from 'crypto';

const OPENAI_KEY = process.env.OPENAI_API_KEY;
const OPENAI_BASE = process.env.OPENAI_BASE || 'https://api.openai.com/v1';
const MOCK = process.env.OPENAI_MOCK === 'true';

// Simple in-memory cache (replace with durable store in production)
const inMemoryCache = new Map<string, any>();

/**
 * Check if we should use mock mode
 */
function shouldUseMock(mockKey?: string): boolean {
  return !!(mockKey || MOCK || !OPENAI_KEY);
}

/**
 * Generate a context-aware mock chat response
 */
function generateMockChatResponse(messages: any[], stream: boolean = false): any {
  const lastMessage = messages[messages.length - 1]?.content || '';
  const isQuestion = lastMessage.includes('?');
  const isExplanation = lastMessage.toLowerCase().includes('explain');
  
  let mockContent = 'This is a mock response.';
  
  if (isQuestion) {
    mockContent = 'Mock answer: This is a demo response. In production, this would be generated by OpenAI.';
  } else if (isExplanation) {
    mockContent = 'Mock explanation: This feature demonstrates the OpenAI integration. When the API key is configured, you\'ll receive real AI-generated responses.';
  } else if (lastMessage.toLowerCase().includes('quaternion')) {
    mockContent = 'Mock response: Quaternion is a strategic game featuring AI commanders, resource management, and tactical combat. This is a demo response.';
  } else {
    mockContent = `Mock response for: "${lastMessage.substring(0, 50)}..." - This is a demo. Configure OPENAI_API_KEY for real responses.`;
  }

  if (stream) {
    // For streaming, return a simple mock stream
    const mockStream = new ReadableStream({
      start(controller) {
        const chunks = mockContent.split(' ');
        let index = 0;
        const interval = setInterval(() => {
          if (index < chunks.length) {
            const chunk = `data: ${JSON.stringify({
              id: 'mock',
              choices: [{
                delta: { content: chunks[index] + (index < chunks.length - 1 ? ' ' : '') },
                index: 0
              }]
            })}\n\n`;
            controller.enqueue(new TextEncoder().encode(chunk));
            index++;
          } else {
            controller.enqueue(new TextEncoder().encode('data: [DONE]\n\n'));
            controller.close();
            clearInterval(interval);
          }
        }, 50); // 50ms delay between chunks
      }
    });
    return mockStream;
  }

  return {
    id: 'mock-' + Date.now(),
    object: 'chat.completion',
    created: Math.floor(Date.now() / 1000),
    model: 'gpt-4o-mini',
    choices: [{
      index: 0,
      message: {
        role: 'assistant',
        content: mockContent
      },
      finish_reason: 'stop'
    }],
    usage: {
      prompt_tokens: 10,
      completion_tokens: 20,
      total_tokens: 30
    }
  };
}

/**
 * Generate mock transcription response
 */
function generateMockTranscription(): any {
  return {
    text: 'Mock transcription: This is a demo response. Configure OPENAI_API_KEY for real audio transcription.',
    language: 'en',
    duration: 2.5
  };
}

function sha256hex(str: string): string {
  return crypto.createHash('sha256').update(str).digest('hex');
}

// Exponential backoff helper
async function retryWithBackoff<T>(
  fn: () => Promise<T>,
  { retries = 3, minDelay = 200, factor = 2 }: { retries?: number; minDelay?: number; factor?: number } = {}
): Promise<T> {
  let attempt = 0;
  let delay = minDelay;
  while (true) {
    try {
      return await fn();
    } catch (err) {
      attempt++;
      if (attempt > retries) throw err;
      await new Promise(r => setTimeout(r, delay));
      delay *= factor;
    }
  }
}

// Proxy fetch wrapper for OpenAI
async function openaiFetch(path: string, opts: RequestInit = {}): Promise<Response> {
  if (!OPENAI_KEY) {
    throw new Error('Missing OPENAI_API_KEY');
  }
  const url = `${OPENAI_BASE}${path}`;
  const headers = {
    Authorization: `Bearer ${OPENAI_KEY}`,
    ...(opts.headers || {}),
  };
  return await retryWithBackoff(() => fetch(url, { ...opts, headers }), { retries: 3 });
}

export async function POST(req: Request) {
  try {
    const url = new URL(req.url);
    const pathname = url.pathname.replace(/\/+$/, '');

    // 1) Chat (supports streaming if body.stream === true)
    // Handles routes: /ai/openai/chat or /api/ai/openai/chat
    if ((pathname.endsWith('/ai/openai/chat') || pathname.includes('/ai/openai/chat')) && req.method === 'POST') {
      const body = await req.json();
      const { mockKey, messages = [], stream = false } = body;
      
      // Check if we should use mock mode
      if (shouldUseMock(mockKey)) {
        const mockResponse = generateMockChatResponse(messages, stream);
        
        if (stream) {
          return new Response(mockResponse, {
            status: 200,
            headers: { 'Content-Type': 'text/event-stream' },
          });
        }
        
        return new Response(JSON.stringify(mockResponse), {
          status: 200,
          headers: { 'Content-Type': 'application/json' },
        });
      }

      // Caching by prompt hash (optional)
      const promptHash = sha256hex(JSON.stringify(body));
      if (body.cache && inMemoryCache.has(promptHash)) {
        const cached = inMemoryCache.get(promptHash);
        return new Response(JSON.stringify(cached), {
          status: 200,
          headers: { 'Content-Type': 'application/json' },
        });
      }

      // Build OpenAI request payload - use /chat/completions
      const payload = {
        model: body.model || 'gpt-4o-mini', // pick your model
        messages: body.messages || [],
        max_tokens: body.max_tokens ?? 512,
        temperature: body.temperature ?? 0.8,
        stream: !!body.stream,
      };

      if (payload.stream) {
        // Stream: fetch streaming response and forward as text/event-stream
        let resp: Response;
        try {
          resp = await openaiFetch('/chat/completions', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(payload),
          });
        } catch (err: any) {
          // Fallback to mock on fetch error
          console.warn('OpenAI API error, falling back to mock:', err.message);
          const mockStream = generateMockChatResponse(messages, true);
          return new Response(mockStream, {
            status: 200,
            headers: { 'Content-Type': 'text/event-stream' },
          });
        }

        if (!resp.ok) {
          // Fallback to mock on API error
          const txt = await resp.text().catch(() => 'Unknown error');
          console.warn(`OpenAI API error ${resp.status}, falling back to mock: ${txt}`);
          const mockStream = generateMockChatResponse(messages, true);
          return new Response(mockStream, {
            status: 200,
            headers: { 'Content-Type': 'text/event-stream' },
          });
        }

        // Pass the streaming body through to the client
        const reader = resp.body?.getReader();
        if (!reader) {
          return new Response('No stream available', { status: 500 });
        }

        const stream = new ReadableStream({
          async pull(controller) {
            const { done, value } = await reader.read();
            if (done) {
              controller.close();
              return;
            }
            controller.enqueue(value);
          },
        });
        
        return new Response(stream, {
          status: 200,
          headers: { 'Content-Type': 'text/event-stream' },
        });
      } else {
        // Non-streaming
        let resp: Response;
        try {
          resp = await openaiFetch('/chat/completions', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(payload),
          });
        } catch (err: any) {
          // Fallback to mock on fetch error
          console.warn('OpenAI API error, falling back to mock:', err.message);
          const mockResponse = generateMockChatResponse(messages, false);
          return new Response(JSON.stringify(mockResponse), {
            status: 200,
            headers: { 'Content-Type': 'application/json' },
          });
        }
        
        if (!resp.ok) {
          // Fallback to mock on API error
          const txt = await resp.text().catch(() => 'Unknown error');
          console.warn(`OpenAI API error ${resp.status}, falling back to mock: ${txt}`);
          const mockResponse = generateMockChatResponse(messages, false);
          return new Response(JSON.stringify(mockResponse), {
            status: 200,
            headers: { 'Content-Type': 'application/json' },
          });
        }
        
        const json = await resp.json();
        if (body.cache) {
          inMemoryCache.set(promptHash, json);
        }
        
        return new Response(JSON.stringify(json), {
          status: 200,
          headers: { 'Content-Type': 'application/json' },
        });
      }
    }

    // 2) Audio transcription (accepts base64 in body)
    // Handles routes: /ai/openai/transcribe or /api/ai/openai/transcribe
    if ((pathname.endsWith('/ai/openai/transcribe') || pathname.includes('/ai/openai/transcribe')) && req.method === 'POST') {
      const body = await req.json();
      const { mockKey } = body;
      
      // Check if we should use mock mode
      if (shouldUseMock(mockKey)) {
        return new Response(
          JSON.stringify(generateMockTranscription()),
          {
            status: 200,
            headers: { 'Content-Type': 'application/json' },
          }
        );
      }

      // OpenAI Audio transcription endpoint: /audio/transcriptions
      // Build a multipart/form-data body
      const audioBuffer = Buffer.from(body.audioBase64, 'base64');
      const formData = new FormData();
      formData.append('file', new Blob([audioBuffer]), 'recording.wav');
      formData.append('model', body.model || 'whisper-1');

      // Note: in Edge runtimes FormData + fetch works; adapt if not available
      let resp: Response;
      try {
        resp = await openaiFetch('/audio/transcriptions', {
          method: 'POST',
          body: formData,
        });
      } catch (err: any) {
        // Fallback to mock on fetch error
        console.warn('OpenAI transcription error, falling back to mock:', err.message);
        return new Response(
          JSON.stringify(generateMockTranscription()),
          {
            status: 200,
            headers: { 'Content-Type': 'application/json' },
          }
        );
      }
      
      if (!resp.ok) {
        // Fallback to mock on API error
        const txt = await resp.text().catch(() => 'Unknown error');
        console.warn(`OpenAI transcription error ${resp.status}, falling back to mock: ${txt}`);
        return new Response(
          JSON.stringify(generateMockTranscription()),
          {
            status: 200,
            headers: { 'Content-Type': 'application/json' },
          }
        );
      }
      
      const json = await resp.json();
      return new Response(JSON.stringify(json), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    return new Response('Not found', { status: 404 });
  } catch (err: any) {
    console.error('openai edge error', err);
    // Always return mock data on unexpected errors (graceful degradation)
    try {
      const url = new URL(req.url);
      const pathname = url.pathname.replace(/\/+$/, '');
      
      if (pathname.includes('/ai/openai/chat')) {
        // Try to get body, but don't fail if we can't
        let body: any = {};
        try {
          body = await req.json();
        } catch {
          // If we can't parse body, use defaults
        }
        const mockResponse = generateMockChatResponse(body.messages || [], body.stream || false);
        if (body.stream) {
          return new Response(mockResponse, {
            status: 200,
            headers: { 'Content-Type': 'text/event-stream' },
          });
        }
        return new Response(JSON.stringify(mockResponse), {
          status: 200,
          headers: { 'Content-Type': 'application/json' },
        });
      }
      
      if (pathname.includes('/ai/openai/transcribe')) {
        return new Response(
          JSON.stringify(generateMockTranscription()),
          {
            status: 200,
            headers: { 'Content-Type': 'application/json' },
          }
        );
      }
    } catch (fallbackErr) {
      // If even fallback fails, return error
    }
    
    return new Response(JSON.stringify({ error: err.message }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    });
  }
}

